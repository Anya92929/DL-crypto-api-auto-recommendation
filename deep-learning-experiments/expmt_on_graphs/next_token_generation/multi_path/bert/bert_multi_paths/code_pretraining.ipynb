{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from bert-tensorflow) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0419 15:36:43.464694 140736186118976 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "#from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import modeling\n",
    "#from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = '../output'\n",
    "\n",
    "\n",
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "NUM_TRAIN_STEPS = 3\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 50\n",
    "SAVE_SUMMARY_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_indexes(sequence_tensor, positions):\n",
    "    \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "    sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "    \n",
    "    batch_size = sequence_shape[0]\n",
    "    seq_length = sequence_shape[1]\n",
    "    width = sequence_shape[2]\n",
    "    \n",
    "    flat_offsets = tf.reshape(\n",
    "        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "    flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "    flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                    [batch_size * seq_length, width])\n",
    "    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lm_output(bert_config, input_tensor, output_weights, positions,\n",
    "                         label_ids, label_weights):\n",
    "    \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "    input_tensor = gather_indexes(input_tensor, positions)\n",
    "    \n",
    "    with tf.variable_scope(\"cls/predictions\"):\n",
    "        # We apply one more non-linear transformation before the output layer.\n",
    "        # This matrix is not used after pre-training.\n",
    "        with tf.variable_scope(\"transform\"):\n",
    "            input_tensor = tf.layers.dense(\n",
    "                input_tensor,\n",
    "                units=bert_config.hidden_size,\n",
    "                activation=modeling.get_activation(bert_config.hidden_act),\n",
    "                kernel_initializer=modeling.create_initializer(\n",
    "                    bert_config.initializer_range))\n",
    "            input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        output_bias = tf.get_variable(\n",
    "            \"output_bias\",\n",
    "            shape=[bert_config.vocab_size],\n",
    "            initializer=tf.zeros_initializer())\n",
    "        logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        \n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        \n",
    "        label_ids = tf.reshape(label_ids, [-1])\n",
    "        label_weights = tf.reshape(label_weights, [-1])\n",
    "\n",
    "        one_hot_labels = tf.one_hot(\n",
    "            label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "        # The `positions` tensor might be zero-padded (if the sequence is too\n",
    "        # short to have the maximum number of predictions). The `label_weights`\n",
    "        # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
    "        # padding predictions.\n",
    "        per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "        numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "        denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "        loss = numerator / denominator\n",
    "    \n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(is_training, input_ids, input_mask, segment_ids, labels):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "    \n",
    "    (lm_loss, lm_predicted_labels, lm_log_probs) = get_lm_output(\n",
    "         bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "         masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "    if is_training:\n",
    "        return (lm_loss, lm_predicted_labels, lm_log_probs)\n",
    "    else:\n",
    "        return (lm_predicted_labels,log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(bert_config, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "        \n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                is_training, input_ids, input_mask, segment_ids, label_ids)\n",
    "        \n",
    "            train_op = optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "        \n",
    "             #Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                f1_score = tf.contrib.metrics.f1_score(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                auc = tf.metrics.auc(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"f1_score\": f1_score,\n",
    "                    \"auc\": auc,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "        \n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    train_op=train_op)\n",
    "            else:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                    loss=loss,\n",
    "                    eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(\n",
    "                is_training, input_ids, input_mask, segment_ids, label_ids)\n",
    "            predictions = {\n",
    "              'probabilities': log_probs,\n",
    "              'labels': predicted_labels\n",
    "            }\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0419 15:36:58.858961 140736186118976 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file('./bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_reader(input_file, max_seq_length, max_predictions_per_seq=1):\n",
    "    # read out the raw input\n",
    "    with open(input_file) as file:\n",
    "        reader = csv.reader(file)\n",
    "        sentences = [[int(i) for i in row] for row in reader]\n",
    "    keys = ['input_ids','mask_ids','segment_ids', 'masked_lm_positions', 'masked_lm_weights']\n",
    "    features = dict([(key, []) for key in keys])\n",
    "    \n",
    "    for row in sentences:\n",
    "        length = min(len(row),max_seq_length)\n",
    "        sentence = row[len(row)-length:]\n",
    "        input_ids = np.pad(row,(0,max_seq_length-len(sentence)),'constant', constant_values=(0, 0))\n",
    "        mask_ids = np.ones(len(sentence))\n",
    "        mask_ids = np.pad(mask_ids,(0,max_seq_length-len(sentence)),'constant',constant_values=(0, 0))\n",
    "        segment_ids = np.ones(max_seq_length)\n",
    "        masked_lm_positions = np.array(len(sentence)-1)\n",
    "        masked_lm_weights = np.array(1.0)\n",
    "        \n",
    "        features['input_ids'].append(input_ids)\n",
    "        features['mask_ids'].append(mask_ids)\n",
    "        features['segment_ids'].append(segment_ids)\n",
    "        features['masked_lm_positions'].append(masked_lm_positions)\n",
    "        features['masked_lm_weights'].append(masked_lm_weights)\n",
    "    return features\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn_builder(feature_reader,\n",
    "                     is_training):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "    \n",
    "    def input_fn(params):\n",
    "        \"\"\"An input function for training or evaluating\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        # Convert the inputs to a Dataset.\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(feature_reader))\n",
    "\n",
    "        # Shuffle and repeat if you are in training mode.\n",
    "        if training:\n",
    "            dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "        return dataset.batch(batch_size)\n",
    "\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = '../../data/tasks/dep2vec/train/dependency_traces_training_ordered.csv'\n",
    "feature_reader = data_reader(input_file,10)\n",
    "train_input_fn = input_fn_builder(feature_reader, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(feature_reader['input_ids']) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456632\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_reader['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fn = model_fn_builder(bert_config,  \n",
    "                            learning_rate=LEARNING_RATE,\n",
    "                            num_train_steps=num_train_steps, \n",
    "                            num_warmup_steps=num_warmup_steps, \n",
    "                            use_tpu=False,\n",
    "                            use_one_hot_embeddings=True)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = '../../data/tasks/dep2vec/train/dependency_traces_training_ordered.csv'\n",
    "feature_reader = data_reader(input_file,10)\n",
    "train_input_fn = input_fn_builder(feature_reader, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0419 15:37:53.359685 140736186118976 deprecation.py:323] From /Users/arielxiao/Library/Python/3.7/lib/python/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "#current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "#print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
